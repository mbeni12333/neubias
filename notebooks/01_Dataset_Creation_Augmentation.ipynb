{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§  Introduction to Patch Datasets in Pathology\n",
    "\n",
    "Brief overview of why we work with patches (due to the large size of Whole Slide Images - WSIs), common challenges (stain variation, artifacts), and the importance of proper dataset handling in computational pathology for tasks like classification or segmentation. Explain that this notebook focuses on creating a robust data pipeline for patch-level classification.\n",
    "\n",
    "## Notebook Plan\n",
    "\n",
    "This notebook will guide you through the essential steps of preparing a patch-based image dataset for a classification task:\n",
    "\n",
    "1.  **Setup & Configuration:** Import necessary libraries (PyTorch, Lightning, Albumentations, scikit-learn, Plotly) and configure paths and class names for our dataset.\n",
    "2.  **Exploring Augmentation (Albumentations):**\n",
    "    *   Load a sample image.\n",
    "    *   Introduce `albumentations` and demonstrate various image transforms (flips, rotation, color jitter).\n",
    "    *   Visualize the effect of these transforms on the sample image.\n",
    "3.  **Defining Data Transforms:** Create specific augmentation pipelines using `albumentations.Compose` for the training set (including augmentations) and the validation/test sets (typically only normalization and tensor conversion).\n",
    "4.  **Implementing the `PatchDataset`:** Build a custom `torch.utils.data.Dataset` class capable of:\n",
    "    *   Loading image paths and corresponding labels.\n",
    "    *   Applying the defined transforms.\n",
    "    *   Returning image tensors and label indices.\n",
    "5.  **Stratified Data Splitting:** Use `sklearn.model_selection.train_test_split` to divide the dataset into training, validation, and test sets while preserving the original class distribution (stratification).\n",
    "6.  **Creating Datasets:** Instantiate the `PatchDataset` for each split (train, validation, test) using the appropriate file lists and transforms.\n",
    "7.  **Implementing `DataLoader`:**\n",
    "    *   Explain the role of `torch.utils.data.DataLoader` for batching, shuffling, and parallel loading.\n",
    "    *   Create `DataLoader` instances for the train and validation sets.\n",
    "8.  **Visualizing a Batch:** Define and use a helper function to load a batch from the `DataLoader` and display the images (with denormalization) in a grid to verify the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Common Imports ---\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2 # Important for Albumentations with PyTorch\n",
    "from PIL import Image # For loading images\n",
    "import cv2 # Often used by Albumentations backend\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Visualization (using Plotly) ---\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Set default Plotly template\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "# --- Reproducibility ---\n",
    "# Set random seeds for reproducibility\n",
    "pl.seed_everything(42, workers=True)\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define the path to your raw dataset\n",
    "# Ensure your dataset (e.g., NSCLC IHC images) is in this directory\n",
    "# with subfolders for each class ('TER', 'Necrotic', etc.)\n",
    "DATA_DIR = Path('../data/raw/patch_classification_dataset')\n",
    "\n",
    "# Automatically get class names from subfolder names\n",
    "try:\n",
    "    CLASS_NAMES = sorted([p.name for p in DATA_DIR.glob('*') if p.is_dir()])\n",
    "    if not CLASS_NAMES:\n",
    "        raise FileNotFoundError # Handle case where directory exists but is empty or has no subdirs\n",
    "    NUM_CLASSES = len(CLASS_NAMES)\n",
    "\n",
    "    print(f'Dataset directory: {DATA_DIR}')\n",
    "    print(f'Found {NUM_CLASSES} classes: {CLASS_NAMES}')\n",
    "\n",
    "    # Create mappings\n",
    "    class_to_idx = {name: i for i, name in enumerate(CLASS_NAMES)}\n",
    "    idx_to_class = {i: name for name, i in class_to_idx.items()}\n",
    "\n",
    "    print(f'Class to index mapping: {class_to_idx}')\n",
    "\n",
    "except FileNotFoundError:\n",
    "     print(f'Error: Dataset directory not found or no class subfolders found at {DATA_DIR}')\n",
    "     print('Please ensure your dataset is placed correctly as per the README instructions.')\n",
    "     # You might want to stop execution here in a real notebook if the data isn't found\n",
    "     CLASS_NAMES = []\n",
    "     NUM_CLASSES = 0\n",
    "     class_to_idx = {}\n",
    "     idx_to_class = {}\n",
    "\n",
    "# Remember: Keep code clean, well-commented, and follow best practices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”„ Albumentations: Transforms & Visualization\n",
    "\n",
    "Data augmentation is crucial for training robust deep learning models, especially when dealing with limited medical imaging data. Albumentations is a powerful library specifically designed for fast and flexible image augmentations.\n",
    "\n",
    "Let's explore some common transformations and visualize their effects on a single sample image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load a single sample image for demonstration ---\n",
    "sample_image_path = None\n",
    "if CLASS_NAMES and DATA_DIR.exists():\n",
    "    first_class_dir = DATA_DIR / CLASS_NAMES[0]\n",
    "    try:\n",
    "        # Find the first PNG image in the first class directory\n",
    "        sample_image_path = next(first_class_dir.glob('*.png'))\n",
    "        print(f\"Loading sample image: {sample_image_path}\")\n",
    "        sample_image_pil = Image.open(sample_image_path).convert('RGB')\n",
    "        sample_image_np = np.array(sample_image_pil)\n",
    "\n",
    "        # Visualize the original sample\n",
    "        fig = px.imshow(sample_image_np, title=f\"Original Sample ({CLASS_NAMES[0]})\")\n",
    "        fig.update_layout(coloraxis_showscale=False).update_xaxes(showticklabels=False).update_yaxes(showticklabels=False)\n",
    "        fig.show()\n",
    "\n",
    "    except StopIteration:\n",
    "        print(f\"Error: No PNG images found in the first class directory: {first_class_dir}\")\n",
    "        sample_image_np = None # Ensure variable exists but is None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading sample image {sample_image_path}: {e}\")\n",
    "        sample_image_np = None\n",
    "else:\n",
    "    print(\"Cannot load sample image: Data directory or class subfolders not found or empty.\")\n",
    "    sample_image_np = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define some example Albumentations transforms ---\n",
    "\n",
    "# 1. Simple Horizontal Flip\n",
    "transform_flip = A.Compose([\n",
    "    A.HorizontalFlip(p=1.0), # p=1.0 means always apply\n",
    "])\n",
    "\n",
    "# 2. Rotation\n",
    "transform_rotate = A.Compose([\n",
    "    A.Rotate(limit=45, p=1.0, border_mode=cv2.BORDER_CONSTANT, value=0), # Rotate up to 45 degrees\n",
    "])\n",
    "\n",
    "# 3. Color Jitter\n",
    "transform_color = A.Compose([\n",
    "    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, p=1.0),\n",
    "])\n",
    "\n",
    "# 4. Combined Transforms (more realistic)\n",
    "# Note: We add ToTensorV2() at the end when using with PyTorch/Lightning\n",
    "transform_combined = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.Rotate(limit=30, p=0.5, border_mode=cv2.BORDER_CONSTANT, value=0),\n",
    "    A.RandomBrightnessContrast(p=0.3),\n",
    "    A.GaussNoise(p=0.2),\n",
    "    # IMPORTANT: Resize if your model expects a fixed input size\n",
    "    # A.Resize(height=224, width=224),\n",
    "    # A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # Example: ImageNet normalization\n",
    "    # ToTensorV2() # Converts image to PyTorch tensor (CHW format) - ADD THIS LATER IN DATASET/DATAMODULE\n",
    "])\n",
    "\n",
    "\n",
    "# --- Apply transforms to the sample image ---\n",
    "if sample_image_np is not None:\n",
    "    augmented_flip = transform_flip(image=sample_image_np)['image']\n",
    "    augmented_rotate = transform_rotate(image=sample_image_np)['image']\n",
    "    augmented_color = transform_color(image=sample_image_np)['image']\n",
    "    augmented_combined = transform_combined(image=sample_image_np)['image']\n",
    "    print(\"Applied various transformations to the sample image.\")\n",
    "else:\n",
    "    print(\"Skipping transform application as sample image failed to load.\")\n",
    "    # Assign None to prevent errors in the next cell\n",
    "    augmented_flip, augmented_rotate, augmented_color, augmented_combined = None, None, None, None\n",
    "\n",
    "\n",
    "# --- Visualize the results ---\n",
    "if sample_image_np is not None and augmented_combined is not None:\n",
    "    fig = make_subplots(rows=1, cols=5, subplot_titles=(\"Original\", \"Flipped\", \"Rotated\", \"Color Jitter\", \"Combined\"))\n",
    "\n",
    "    # Add original image\n",
    "    fig.add_trace(px.imshow(sample_image_np).data[0], row=1, col=1)\n",
    "    # Add augmented images\n",
    "    fig.add_trace(px.imshow(augmented_flip).data[0], row=1, col=2)\n",
    "    fig.add_trace(px.imshow(augmented_rotate).data[0], row=1, col=3)\n",
    "    fig.add_trace(px.imshow(augmented_color).data[0], row=1, col=4)\n",
    "    fig.add_trace(px.imshow(augmented_combined).data[0], row=1, col=5)\n",
    "\n",
    "    fig.update_layout(title_text=\"Albumentations Examples\", height=400, width=1200)\n",
    "    fig.update_xaxes(showticklabels=False)\n",
    "    fig.update_yaxes(showticklabels=False)\n",
    "    fig.update_layout(coloraxis_showscale=False)\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"Cannot visualize augmentations because the sample image or augmented images are missing.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've applied several common augmentations:\n",
    "*   **Geometric:** `HorizontalFlip`, `VerticalFlip`, `Rotate`. These help the model become invariant to object orientation.\n",
    "*   **Color:** `ColorJitter`, `RandomBrightnessContrast`. These help the model generalize to variations in staining and illumination.\n",
    "*   **Noise:** `GaussNoise`. Can improve robustness.\n",
    "\n",
    "**Important Considerations:**\n",
    "*   **`p` parameter:** Controls the probability of applying a transform (e.g., `p=0.5` means 50% chance).\n",
    "*   **`Compose`:** Chains multiple transforms together. The order can sometimes matter.\n",
    "*   **`ToTensorV2()`:** This essential transform converts the NumPy array (HWC format, 0-255) to a PyTorch tensor (CHW format, usually 0-1) and should typically be the *last* transform in the sequence when preparing data for PyTorch models. We will add it later when integrating with our `Dataset`.\n",
    "*   **Normalization:** Often applied after `ToTensorV2`. Standard practice is to use statistics (mean, std dev) from large datasets like ImageNet, or calculate them from your own dataset.\n",
    "\n",
    "Now that we understand how Albumentations works, we can integrate these transforms into our `PatchDataset` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¦ PyTorch Dataset Implementation\n",
    "\n",
    "Here we will implement a custom `torch.utils.data.Dataset` class to load our patch images and corresponding labels efficiently. This class will handle finding image paths, loading images, applying transformations (if any), and returning image-label pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Transforms ---\n",
    "\n",
    "# Define image size if resizing is needed\n",
    "# IMG_HEIGHT = 224\n",
    "# IMG_WIDTH = 224\n",
    "\n",
    "# Define normalization constants (e.g., ImageNet stats)\n",
    "# Calculate these from your specific dataset for potentially better results if needed\n",
    "IMG_MEAN = [0.485, 0.456, 0.406]\n",
    "IMG_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Define transforms for the training set (including augmentation)\n",
    "train_transforms = A.Compose([\n",
    "    # A.Resize(height=IMG_HEIGHT, width=IMG_WIDTH), # Uncomment if resizing needed\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.Rotate(limit=30, p=0.5, border_mode=cv2.BORDER_CONSTANT, value=0),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.3),\n",
    "    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.3), # Color jitter alternative\n",
    "    A.GaussNoise(var_limit=(10.0, 50.0), p=0.2),\n",
    "    # Normalization should come after color augmentations and before ToTensorV2\n",
    "    A.Normalize(mean=IMG_MEAN, std=IMG_STD),\n",
    "    ToTensorV2(), # Converts image to PyTorch tensor (CHW format) and scales to [0, 1] if input is uint8\n",
    "])\n",
    "\n",
    "# Define transforms for validation and test sets (usually no augmentation, just normalization and tensor conversion)\n",
    "val_test_transforms = A.Compose([\n",
    "    # A.Resize(height=IMG_HEIGHT, width=IMG_WIDTH), # Uncomment if resizing needed\n",
    "    A.Normalize(mean=IMG_MEAN, std=IMG_STD),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "print(\"Defined train_transforms and val_test_transforms.\")\n",
    "\n",
    "# Optional: Visualize an image after applying train_transforms\n",
    "if sample_image_np is not None:\n",
    "    transformed_sample = train_transforms(image=sample_image_np)['image']\n",
    "    print(\"Sample image shape after train_transforms (should be CHW tensor):\", transformed_sample.shape)\n",
    "    # Note: Visualization of the normalized tensor might look strange directly with imshow\n",
    "    # We might need to denormalize or just show one channel if needed later.\n",
    "else:\n",
    "    print(\"Cannot apply train_transforms as sample_image_np is None.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Modified PatchDataset Class ---\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "class PatchDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for patch classification.\n",
    "    Accepts a list of filepaths and corresponding labels.\n",
    "    \"\"\"\n",
    "    def __init__(self, filepaths, labels, transform=None, label_map=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            filepaths (list): List of paths to image files.\n",
    "            labels (list): List of corresponding labels (strings).\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "            label_map (dict, optional): Dictionary mapping string labels to integer indices.\n",
    "        \"\"\"\n",
    "        self.filepaths = filepaths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "        # Create or use label map\n",
    "        if label_map:\n",
    "            self.label_map = label_map\n",
    "        else:\n",
    "            # Create map from unique labels found\n",
    "            unique_labels = sorted(list(set(labels)))\n",
    "            self.label_map = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "\n",
    "        self.idx_to_label = {v: k for k, v in self.label_map.items()} # For potential reverse lookup\n",
    "\n",
    "        print(f\"Initialized Dataset. Label map: {self.label_map}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filepaths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.filepaths[idx]\n",
    "        label_str = self.labels[idx]\n",
    "        label_idx = self.label_map[label_str]\n",
    "\n",
    "        # Load image using PIL (ensure RGB)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        # Apply transforms if they exist\n",
    "        if self.transform:\n",
    "            # Albumentations requires numpy array\n",
    "            image_np = np.array(image)\n",
    "            augmented = self.transform(image=image_np)\n",
    "            image_tensor = augmented['image'] # Albumentations returns a dict\n",
    "        else:\n",
    "            # Basic transform to tensor if no augmentation\n",
    "            # Note: Albumentations ToTensorV2 normalizes by default,\n",
    "            # this basic conversion does not. Add normalization if needed.\n",
    "            image_tensor = torch.from_numpy(np.array(image)).permute(2, 0, 1).float() / 255.0\n",
    "\n",
    "        return image_tensor, label_idx\n",
    "\n",
    "    def get_label_map(self):\n",
    "        return self.label_map\n",
    "\n",
    "    def get_idx_to_label(self):\n",
    "        return self.idx_to_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Stratified Train/Validation/Test Split ---\n",
    "\n",
    "# Define split ratios\n",
    "TRAIN_RATIO = 0.70\n",
    "VAL_RATIO = 0.15\n",
    "TEST_RATIO = 0.15 # VAL_RATIO + TEST_RATIO should equal 1.0 - TRAIN_RATIO\n",
    "\n",
    "all_image_paths = []\n",
    "all_labels = []\n",
    "\n",
    "print(\"Scanning for all image paths and labels...\")\n",
    "if DATA_DIR.exists() and CLASS_NAMES:\n",
    "    for class_name, idx in class_to_idx.items():\n",
    "        class_dir = DATA_DIR / class_name\n",
    "        if class_dir.is_dir():\n",
    "            for img_path in glob.glob(str(class_dir / '*.png')): # Adjust pattern if needed\n",
    "               all_image_paths.append(Path(img_path))\n",
    "               all_labels.append(idx)\n",
    "    print(f\"Found {len(all_image_paths)} total images.\")\n",
    "else:\n",
    "    print(\"Error: Cannot perform split. Data directory or class folders not found/empty.\")\n",
    "    # Assign empty lists to prevent errors later\n",
    "    all_image_paths = []\n",
    "    all_labels = []\n",
    "\n",
    "\n",
    "# Perform the split only if images were found\n",
    "if all_image_paths:\n",
    "    # First split: separate train set from (validation + test) set\n",
    "    # Ensure stratify is only used if there are enough samples per class for the split\n",
    "    try:\n",
    "        train_paths, temp_paths, train_labels, temp_labels = train_test_split(\n",
    "            all_image_paths,\n",
    "            all_labels,\n",
    "            test_size=(VAL_RATIO + TEST_RATIO), # Size of the temporary set\n",
    "            random_state=42, # for reproducibility\n",
    "            stratify=all_labels # Ensure class distribution is similar\n",
    "        )\n",
    "\n",
    "        # Second split: separate validation set from test set\n",
    "        # Adjust test_size relative to the *temporary* set size\n",
    "        relative_test_size = TEST_RATIO / (VAL_RATIO + TEST_RATIO)\n",
    "\n",
    "        val_paths, test_paths, val_labels, test_labels = train_test_split(\n",
    "            temp_paths,\n",
    "            temp_labels,\n",
    "            test_size=relative_test_size,\n",
    "            random_state=42, # for reproducibility\n",
    "            stratify=temp_labels # Ensure class distribution is similar\n",
    "        )\n",
    "\n",
    "        print(f\"\\nDataset split:\")\n",
    "        print(f\"  Train samples: {len(train_paths)}\")\n",
    "        print(f\"  Validation samples: {len(val_paths)}\")\n",
    "        print(f\"  Test samples: {len(test_paths)}\")\n",
    "\n",
    "        # Verify stratification (optional but recommended)\n",
    "        from collections import Counter\n",
    "        print(\"\\nClass distribution:\")\n",
    "        print(f\"  Overall: {Counter(all_labels)}\")\n",
    "        print(f\"  Train:   {Counter(train_labels)}\")\n",
    "        print(f\"  Val:     {Counter(val_labels)}\")\n",
    "        print(f\"  Test:    {Counter(test_labels)}\")\n",
    "\n",
    "    except ValueError as e:\n",
    "         print(f\"\\nError during stratified split: {e}\")\n",
    "         print(\"This might happen if a class has too few samples for the requested split ratios.\")\n",
    "         print(\"Assigning all data to train set as fallback (adjust as needed).\")\n",
    "         train_paths, val_paths, test_paths = all_image_paths, [], []\n",
    "         train_labels, val_labels, test_labels = all_labels, [], []\n",
    "\n",
    "else:\n",
    "    print(\"Skipping split as no images were loaded.\")\n",
    "    train_paths, val_paths, test_paths = [], [], []\n",
    "    train_labels, val_labels, test_labels = [], [], []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create Datasets using the split file lists ---\n",
    "\n",
    "# Make sure train_transform and val_test_transform are defined earlier\n",
    "\n",
    "# Check if paths lists are not empty before creating datasets\n",
    "if train_paths:\n",
    "    train_dataset = PatchDataset(\n",
    "        filepaths=train_paths, # CHANGE HERE\n",
    "        labels=train_labels,   # CHANGE HERE\n",
    "        transform=train_transforms\n",
    "    )\n",
    "    # Use the label map from the training set for consistency\n",
    "    label_map = train_dataset.get_label_map()\n",
    "    print(f\"\\nTrain dataset size: {len(train_dataset)}\")\n",
    "else:\n",
    "    train_dataset = None\n",
    "    label_map = {} # Define an empty map if no training data\n",
    "    print(\"\\nWarning: No training data found. Cannot create training dataset.\")\n",
    "\n",
    "if val_paths:\n",
    "    val_dataset = PatchDataset(\n",
    "        filepaths=val_paths,     # CHANGE HERE\n",
    "        labels=val_labels,       # CHANGE HERE\n",
    "        transform=val_test_transforms, # Use non-augmenting transforms for validation\n",
    "        label_map=label_map # Use map from training data\n",
    "    )\n",
    "    print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "else:\n",
    "    val_dataset = None\n",
    "    print(\"Warning: No validation data found. Cannot create validation dataset.\")\n",
    "\n",
    "\n",
    "if test_paths:\n",
    "    test_dataset = PatchDataset(\n",
    "        filepaths=test_paths,    # CHANGE HERE\n",
    "        labels=test_labels,      # CHANGE HERE\n",
    "        transform=val_test_transforms, # Use non-augmenting transforms for test\n",
    "        label_map=label_map # Use map from training data\n",
    "    )\n",
    "    print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "else:\n",
    "    test_dataset = None\n",
    "    print(\"Warning: No test data found. Cannot create test dataset.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸšš PyTorch DataLoader Implementation\n",
    "\n",
    "Now that we have a `Dataset`, we need a `DataLoader` to efficiently load batches of data during training. The `DataLoader` handles several key aspects:\n",
    "\n",
    "*   **Batching:** Grouping individual samples into mini-batches.\n",
    "*   **Shuffling:** Randomly shuffling the data order at each epoch (important for training).\n",
    "*   **Parallel Loading:** Using multiple worker processes (`num_workers`) to load data in the background, preventing the GPU from waiting for data.\n",
    "*   **Memory Pinning:** Optionally pinning memory (`pin_memory=True`) for faster CPU-to-GPU transfers.\n",
    "\n",
    "We will create a basic DataLoader example first and then see how to integrate it properly within a LightningDataModule later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create DataLoaders ---\n",
    "batch_size = 32 # Example batch size\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True) # For later use\n",
    "\n",
    "print(f\"\\nTrain DataLoader batches: {len(train_dataloader)}\")\n",
    "print(f\"Validation DataLoader batches: {len(val_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils\n",
    "\n",
    "def denormalize(tensor, mean=IMG_MEAN, std=IMG_STD):\n",
    "    \"\"\"Denormalizes a tensor image with mean and standard deviation.\"\"\"\n",
    "    # Clone to avoid modifying the original tensor\n",
    "    tensor = tensor.clone()\n",
    "    for t, m, s in zip(tensor, mean, std):\n",
    "        t.mul_(s).add_(m)  # Reverse the normalization: (tensor * std) + mean\n",
    "    # We need to clamp values to [0, 1] after denormalization\n",
    "    tensor = torch.clamp(tensor, 0, 1)\n",
    "    return tensor\n",
    "\n",
    "# --- Visualize a batch from the DataModule's DataLoader ---\n",
    "\n",
    "# Ensure the show_batch_grid function is defined earlier\n",
    "# Make sure it uses the potentially updated idx_to_class_from_module map\n",
    "\n",
    "def show_batch_grid(dataloader, num_images=16, title=\"Sample Batch\", idx_map=None): # Added idx_map\n",
    "    \"\"\"Fetches one batch and displays it using torchvision.utils.make_grid and matplotlib.\"\"\"\n",
    "    if not dataloader:\n",
    "        print(\"DataLoader is None, cannot show batch.\")\n",
    "        return\n",
    "    if not idx_map:\n",
    "        print(\"Index-to-class map not provided.\")\n",
    "        idx_map = {} # Default to empty map\n",
    "\n",
    "    try:\n",
    "        images, labels = next(iter(dataloader))\n",
    "    except StopIteration:\n",
    "        print(\"DataLoader is empty or exhausted.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching batch: {e}\")\n",
    "        return\n",
    "\n",
    "    images_to_show = images[:num_images]\n",
    "    labels_to_show = labels[:num_images]\n",
    "    denormalized_images = [denormalize(img) for img in images_to_show]\n",
    "    grid = torchvision.utils.make_grid(denormalized_images, nrow=int(math.sqrt(num_images)))\n",
    "    grid_np = grid.numpy()\n",
    "    grid_display = np.transpose(grid_np, (1, 2, 0))\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(grid_display)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Labels for displayed batch:\")\n",
    "    print([idx_map.get(l.item(), \"Unknown\") for l in labels_to_show])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš¡ PyTorch Lightning DataModule\n",
    "\n",
    "To streamline the data loading process and integrate seamlessly with PyTorch Lightning's `Trainer`, we encapsulate all the logic (finding files, splitting, creating datasets, defining dataloaders) into a `LightningDataModule`.\n",
    "\n",
    "**Key Benefits:**\n",
    "*   **Organization:** Keeps all data-related code in one place.\n",
    "*   **Reproducibility:** Ensures data splitting and loading are consistent.\n",
    "*   **Decoupling:** Separates data logic from the model definition (`LightningModule`).\n",
    "*   **Trainer Integration:** The `Trainer` automatically calls the appropriate methods (`prepare_data`, `setup`, `train_dataloader`, etc.).\n",
    "\n",
    "We will define a `PatchDataModule` that handles our specific patch classification dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LightningDataModule Implementation ---\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob # Ensure glob is imported if not already in this cell context\n",
    "from pathlib import Path # Ensure Path is imported\n",
    "\n",
    "class PatchDataModule(pl.LightningDataModule):\n",
    "    def __init__(self,\n",
    "                 data_dir: str,\n",
    "                 class_to_idx: dict,\n",
    "                 train_transform: A.Compose,\n",
    "                 val_test_transform: A.Compose,\n",
    "                 batch_size: int = 32,\n",
    "                 num_workers: int = 4,\n",
    "                 train_ratio: float = 0.7,\n",
    "                 val_ratio: float = 0.15,\n",
    "                 test_ratio: float = 0.15,\n",
    "                 seed: int = 42):\n",
    "        super().__init__()\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "        self.train_transform = train_transform\n",
    "        self.val_test_transform = val_test_transform\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.train_ratio = train_ratio\n",
    "        self.val_ratio = val_ratio\n",
    "        self.test_ratio = test_ratio\n",
    "        self.seed = seed\n",
    "\n",
    "        # Placeholders for datasets and file paths after setup\n",
    "        self.train_paths, self.val_paths, self.test_paths = None, None, None\n",
    "        self.train_labels, self.val_labels, self.test_labels = None, None, None\n",
    "        self.train_dataset, self.val_dataset, self.test_dataset = None, None, None\n",
    "        self.label_map = None # Will be determined by train_dataset\n",
    "\n",
    "        # Save hyperparameters for logging (optional but good practice)\n",
    "        self.save_hyperparameters(ignore=['train_transform', 'val_test_transform', 'class_to_idx']) # Avoid saving complex objects\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Called only on 1 GPU/TPU in distributed settings.\n",
    "        # Use this to download data, check existence etc.\n",
    "        if not self.data_dir.exists():\n",
    "             raise FileNotFoundError(f\"Data directory not found: {self.data_dir}\")\n",
    "        print(f\"Data directory check passed: {self.data_dir}\")\n",
    "        # We could add more checks here if needed\n",
    "\n",
    "    def setup(self, stage: str = None):\n",
    "        # Called on every GPU/TPU in distributed settings.\n",
    "        # Assign train/val/test datasets for use in dataloaders\n",
    "        # `stage` can be 'fit', 'validate', 'test', 'predict'\n",
    "        if not self.train_dataset and not self.val_dataset and not self.test_dataset:\n",
    "            print(f\"Setting up data for stage: {stage}\")\n",
    "\n",
    "            # --- Scan & Split ---\n",
    "            all_image_paths = []\n",
    "            all_labels_str = [] # Use string labels for splitting/map creation\n",
    "            class_names = sorted(list(self.class_to_idx.keys()))\n",
    "\n",
    "            print(\"Scanning for all image paths and string labels...\")\n",
    "            if self.data_dir.exists() and class_names:\n",
    "                 for class_name in class_names:\n",
    "                     class_dir = self.data_dir / class_name\n",
    "                     if class_dir.is_dir():\n",
    "                         for img_path in glob.glob(str(class_dir / '*.png')): # Adjust pattern if needed\n",
    "                            all_image_paths.append(Path(img_path))\n",
    "                            all_labels_str.append(class_name) # Store string label\n",
    "                 print(f\"Found {len(all_image_paths)} total images.\")\n",
    "            else:\n",
    "                 print(\"Error: Cannot perform split. Data directory or class folders not found/empty.\")\n",
    "                 # Handle error or return if necessary\n",
    "                 return\n",
    "\n",
    "            if not all_image_paths:\n",
    "                print(\"No images found, cannot proceed with setup.\")\n",
    "                return\n",
    "\n",
    "            # --- Perform Splits ---\n",
    "            try:\n",
    "                # Split 1: Train vs Temp (Val+Test)\n",
    "                self.train_paths, temp_paths, self.train_labels, temp_labels = train_test_split(\n",
    "                    all_image_paths, all_labels_str, # Use string labels\n",
    "                    test_size=(self.val_ratio + self.test_ratio),\n",
    "                    random_state=self.seed,\n",
    "                    stratify=all_labels_str\n",
    "                )\n",
    "                # Split 2: Val vs Test from Temp\n",
    "                relative_test_size = self.test_ratio / (self.val_ratio + self.test_ratio)\n",
    "                self.val_paths, self.test_paths, self.val_labels, self.test_labels = train_test_split(\n",
    "                    temp_paths, temp_labels, # Use string labels\n",
    "                    test_size=relative_test_size,\n",
    "                    random_state=self.seed,\n",
    "                    stratify=temp_labels\n",
    "                )\n",
    "                print(\"Dataset split completed.\")\n",
    "                print(f\"Train size: {len(self.train_paths)}, Val size: {len(self.val_paths)}, Test size: {len(self.test_paths)}\")\n",
    "\n",
    "            except ValueError as e:\n",
    "                print(f\"Error during stratified split: {e}. Check class distribution and split ratios.\")\n",
    "                # Implement fallback or raise error\n",
    "                return\n",
    "\n",
    "            # --- Instantiate Datasets ---\n",
    "            if self.train_paths:\n",
    "                self.train_dataset = PatchDataset(\n",
    "                    filepaths=self.train_paths,\n",
    "                    labels=self.train_labels,\n",
    "                    transform=self.train_transform\n",
    "                    # Let PatchDataset create the label_map from these string labels\n",
    "                )\n",
    "                self.label_map = self.train_dataset.get_label_map() # Get the map\n",
    "\n",
    "            if self.val_paths and self.label_map is not None:\n",
    "                self.val_dataset = PatchDataset(\n",
    "                    filepaths=self.val_paths,\n",
    "                    labels=self.val_labels,\n",
    "                    transform=self.val_test_transform,\n",
    "                    label_map=self.label_map # Use map from train set\n",
    "                )\n",
    "\n",
    "            if self.test_paths and self.label_map is not None:\n",
    "                self.test_dataset = PatchDataset(\n",
    "                    filepaths=self.test_paths,\n",
    "                    labels=self.test_labels,\n",
    "                    transform=self.val_test_transform,\n",
    "                    label_map=self.label_map # Use map from train set\n",
    "                )\n",
    "\n",
    "            print(\"Datasets instantiated.\")\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        if self.train_dataset:\n",
    "            return DataLoader(self.train_dataset,\n",
    "                              batch_size=self.batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=self.num_workers,\n",
    "                              pin_memory=True,\n",
    "                              persistent_workers=True if self.num_workers > 0 else False) # Good practice\n",
    "        return None\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        if self.val_dataset:\n",
    "            return DataLoader(self.val_dataset,\n",
    "                              batch_size=self.batch_size,\n",
    "                              shuffle=False,\n",
    "                              num_workers=self.num_workers,\n",
    "                              pin_memory=True,\n",
    "                              persistent_workers=True if self.num_workers > 0 else False)\n",
    "        return None\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        if self.test_dataset:\n",
    "            return DataLoader(self.test_dataset,\n",
    "                              batch_size=self.batch_size,\n",
    "                              shuffle=False,\n",
    "                              num_workers=self.num_workers,\n",
    "                              pin_memory=True,\n",
    "                              persistent_workers=True if self.num_workers > 0 else False)\n",
    "        return None\n",
    "\n",
    "    # Helper to get label map easily\n",
    "    def get_label_map(self):\n",
    "        if self.label_map is None and self.train_dataset:\n",
    "             self.label_map = self.train_dataset.get_label_map()\n",
    "        return self.label_map\n",
    "\n",
    "    def get_idx_to_label(self):\n",
    "         label_map = self.get_label_map()\n",
    "         if label_map:\n",
    "             return {v: k for k, v in label_map.items()}\n",
    "         return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Instantiate the DataModule ---\n",
    "\n",
    "# Ensure your transforms and class_to_idx map are defined earlier\n",
    "\n",
    "# Example instantiation (adjust parameters as needed)\n",
    "data_module = PatchDataModule(\n",
    "    data_dir=DATA_DIR,\n",
    "    class_to_idx=class_to_idx, # Use the map created in the config cell\n",
    "    train_transform=train_transforms,\n",
    "    val_test_transform=val_test_transforms,\n",
    "    batch_size=32,\n",
    "    num_workers=4, # Adjust based on your system\n",
    "    train_ratio=TRAIN_RATIO,\n",
    "    val_ratio=VAL_RATIO,\n",
    "    test_ratio=TEST_RATIO\n",
    ")\n",
    "\n",
    "# --- Trigger the setup process ---\n",
    "# This performs the splitting and creates the internal datasets\n",
    "data_module.prepare_data() # Check if data dir exists\n",
    "data_module.setup()        # Perform split and dataset creation\n",
    "\n",
    "# --- Get DataLoaders from the module ---\n",
    "train_dl = data_module.train_dataloader()\n",
    "val_dl = data_module.val_dataloader()\n",
    "test_dl = data_module.test_dataloader() # For later\n",
    "\n",
    "# Verify\n",
    "if train_dl:\n",
    "    print(f\"\\nSuccessfully obtained Train Dataloader with {len(train_dl)} batches.\")\n",
    "if val_dl:\n",
    "    print(f\"Successfully obtained Validation Dataloader with {len(val_dl)} batches.\")\n",
    "\n",
    "# Update idx_to_class map from the DataModule's potential map\n",
    "idx_to_class_from_module = data_module.get_idx_to_label()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_dl:\n",
    "    print(\"\\nVisualizing batch from Training DataLoader (via DataModule):\")\n",
    "    show_batch_grid(train_dl, title=\"Sample Training Batch (DataModule)\", idx_map=idx_to_class_from_module)\n",
    "else:\n",
    "    print(\"\\nCannot visualize training batch as train_dl is None.\")\n",
    "\n",
    "if val_dl:\n",
    "    print(\"\\nVisualizing batch from Validation DataLoader (via DataModule):\")\n",
    "    # Note: Validation batches won't show augmentations like flips/rotations\n",
    "    show_batch_grid(val_dl, title=\"Sample Validation Batch (DataModule)\", idx_map=idx_to_class_from_module)\n",
    "else:\n",
    "    print(\"\\nCannot visualize validation batch as val_dl is None.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Notebook 3: Callbacks, Hooks & Training\n",
    "\n",
    "Objective: Learn about PyTorch Lightning Callbacks, implement built-in and\n",
    "           custom callbacks, set up and run a full training loop using the\n",
    "           Trainer, and export the model using TorchScript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Imports ---\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "import os\n",
    "\n",
    "# Assume DataModule and LightningModule can be imported from previous scripts\n",
    "# You might need to adjust sys.path or package structure for this to work\n",
    "# Example (adjust paths as needed):\n",
    "# from pathlib import Path\n",
    "# import sys\n",
    "# script_dir = Path(__file__).parent\n",
    "# sys.path.append(str(script_dir.parent)) # Add project root to path\n",
    "# from src.01_Dataset_Creation_Augmentation import PatchDataModule, train_transforms, val_test_transforms, class_to_idx # Need to refactor these out or pass config\n",
    "# from src.02_model_training import HistologyClassifier\n",
    "\n",
    "# --- Configuration (Placeholders - Load these properly) ---\n",
    "# These would typically come from config files, CLI args, or imported modules\n",
    "CHECKPOINT_DIR = \"./lightning_checkpoints\"\n",
    "LOG_DIR = \"./lightning_logs\"\n",
    "NUM_CLASSES = 7\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# Example: Assume these are loaded or defined\n",
    "# class_to_idx = {'TER': 0, 'Necrotic': 1, ...} # Load from previous step or config\n",
    "# train_transforms = ... # Load from previous step or config\n",
    "# val_test_transforms = ... # Load from previous step or config\n",
    "# DATA_DIR = '../data/raw/patch_classification_dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ž Introduction to Callbacks\n",
    "\n",
    "Callbacks are self-contained programs that can be added to your PyTorch Lightning\n",
    "`Trainer`. They allow you to add custom logic at various stages of the training\n",
    "process (e.g., at the beginning/end of an epoch, before/after a batch) without\n",
    "cluttering your `LightningModule`.\n",
    "\n",
    "**Why use Callbacks?**\n",
    "- **Modularity:** Keep training logic separate from model definition.\n",
    "- **Reusability:** Easily reuse common logic like checkpointing or early stopping across projects.\n",
    "- **Extensibility:** Hook into specific points in the training loop for monitoring, logging, or other actions.\n",
    "\n",
    "Lightning provides several useful built-in callbacks, and you can easily create\n",
    "your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `ModelCheckpoint`\n",
    "\n",
    "This callback saves your model's weights periodically during training.\n",
    "Key parameters:\n",
    "- `dirpath`: Directory to save checkpoints.\n",
    "- `filename`: Naming pattern for checkpoint files (can include metrics).\n",
    "- `monitor`: Metric to monitor for saving the 'best' model (e.g., 'val_loss_epoch').\n",
    "- `mode`: 'min' or 'max' depending on whether the monitored metric should be minimized or maximized.\n",
    "- `save_top_k`: Save the top 'k' best models according to the monitored metric.\n",
    "- `save_last`: Save the latest model checkpoint at the end of every epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Example configuration for ModelCheckpoint\n",
    "# Saves the best model based on validation accuracy (higher is better)\n",
    "checkpoint_callback_acc = ModelCheckpoint(\n",
    "    dirpath=CHECKPOINT_DIR,\n",
    "    filename='best-model-acc-{epoch:02d}-{val_acc:.2f}',\n",
    "    monitor='val_acc', # Assuming 'val_acc' is logged in LightningModule\n",
    "    mode='max',\n",
    "    save_top_k=1, # Save only the single best model\n",
    "    save_last=True, # Also save the latest model state\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Example configuration for ModelCheckpoint\n",
    "# Saves the best model based on validation loss (lower is better)\n",
    "checkpoint_callback_loss = ModelCheckpoint(\n",
    "    dirpath=CHECKPOINT_DIR,\n",
    "    filename='best-model-loss-{epoch:02d}-{val_loss_epoch:.2f}',\n",
    "    monitor='val_loss_epoch', # Monitor the epoch validation loss\n",
    "    mode='min',\n",
    "    save_top_k=1,\n",
    "    save_last=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"ModelCheckpoint callbacks configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `EarlyStopping`\n",
    "\n",
    "This callback stops training early if a monitored metric stops improving,\n",
    "preventing overfitting and saving computation time.\n",
    "Key parameters:\n",
    "- `monitor`: Metric to monitor (e.g., 'val_loss_epoch').\n",
    "- `mode`: 'min' or 'max'.\n",
    "- `patience`: Number of epochs to wait for improvement before stopping.\n",
    "- `min_delta`: Minimum change in the monitored quantity to qualify as an improvement.\n",
    "- `verbose`: Print messages when stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example configuration for EarlyStopping\n",
    "# Stops training if validation loss doesn't improve for 5 consecutive epochs\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss_epoch',\n",
    "    mode='min',\n",
    "    patience=10, # Increase patience for potentially noisy validation loss\n",
    "    min_delta=0.001,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"EarlyStopping callback configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### `LearningRateMonitor`\n",
    "\n",
    "Automatically logs the learning rate used by the optimizer(s) at each step or epoch.\n",
    "Very useful when using learning rate schedulers.\n",
    "Key parameters:\n",
    "- `logging_interval`: 'step' or 'epoch'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr_monitor_callback = LearningRateMonitor(logging_interval='epoch')\n",
    "\n",
    "print(\"LearningRateMonitor callback configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

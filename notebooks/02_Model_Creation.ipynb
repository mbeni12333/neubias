{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook 2: Model + LightningModule + Logging\n",
    "\n",
    "Objective: Implement model architectures (Custom CNN, ResNet), encapsulate them\n",
    "           in a PyTorch Lightning LightningModule, and set up comprehensive\n",
    "           logging for training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Imports ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import pytorch_lightning as pl\n",
    "from torch.optim import AdamW # Example optimizer\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau # Example scheduler\n",
    "# Add metrics imports later if needed (e.g., from torchmetrics)\n",
    "# --- Imports ---\n",
    "# ... (existing imports) ...\n",
    "import torchmetrics # Make sure torchmetrics is installed (pip install torchmetrics)\n",
    "\n",
    "# Specific metrics (adjust average strategy as needed)\n",
    "from torchmetrics.classification import MulticlassAccuracy, MulticlassPrecision, MulticlassRecall, MulticlassF1Score, MulticlassAUROC\n",
    "\n",
    "# --- Configuration ---\n",
    "# These might be loaded from a config file or CLI args in a real script\n",
    "NUM_CLASSES = 7 # From our NSCLC dataset\n",
    "LEARNING_RATE = 1e-4\n",
    "# ... other relevant configurations ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§  Introduction to Histology Image Modeling\n",
    "\n",
    "Histology image classification involves training models to recognize patterns\n",
    "in tissue samples, often stained (e.g., H&E). Key challenges include:\n",
    "- **Large Image Size:** Whole Slide Images (WSIs) are massive, requiring patch-based approaches.\n",
    "- **Subtle Features:** Distinguishing between classes often relies on fine-grained details.\n",
    "- **Stain Variability:** Differences in staining protocols can affect image appearance.\n",
    "- **Class Imbalance:** Some tissue types might be much rarer than others.\n",
    "\n",
    "**Common Approaches:**\n",
    "- **Binary Classification:** Distinguishing between two states (e.g., tumor vs. normal).\n",
    "- **Multi-Class Classification:** Assigning patches to one of several predefined categories (like our 7 region types).\n",
    "- **Transfer Learning:** Leveraging models pre-trained on large datasets (like ImageNet) and fine-tuning them for the specific histology task (often beneficial).\n",
    "- **Custom Architectures:** Designing CNNs tailored to the specific characteristics of histology data.\n",
    "\n",
    "This section focuses on implementing a custom CNN as a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleHistologyCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A basic Convolutional Neural Network for histology patch classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes: int):\n",
    "        super().__init__()\n",
    "        # Define convolutional layers\n",
    "        # Example: (Input channels, Output channels, Kernel size, Stride, Padding)\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) # Reduces spatial dims by half\n",
    "\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Placeholder for calculating the flattened size dynamically\n",
    "        # We'll need to pass a dummy input through the conv layers once\n",
    "        self._feature_size = None\n",
    "        self._calculate_feature_size() # Calculate size during init\n",
    "\n",
    "        # Define fully connected layers\n",
    "        self.fc1 = nn.Linear(self._feature_size, 128)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5) # Dropout for regularization\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def _calculate_feature_size(self):\n",
    "        \"\"\"Helper to determine the size of the flattened features after conv layers.\"\"\"\n",
    "        # Create a dummy input matching expected dimensions (Batch, Channels, Height, Width)\n",
    "        # Assuming input patches are, for example, 256x256\n",
    "        # Adjust the dummy input size if your patches are different!\n",
    "        dummy_input = torch.randn(1, 3, 256, 256) # Example size\n",
    "        x = self.pool1(self.relu1(self.conv1(dummy_input)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = self.pool3(self.relu3(self.conv3(x)))\n",
    "        # Flatten the output\n",
    "        self._feature_size = x.view(x.size(0), -1).shape[1]\n",
    "        print(f\"Calculated feature size after conv layers: {self._feature_size}\")\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional part\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = self.pool3(self.relu3(self.conv3(x)))\n",
    "\n",
    "        # Flatten the features\n",
    "        x = x.view(x.size(0), -1) # Flatten all dimensions except batch\n",
    "\n",
    "        # Fully connected part\n",
    "        x = self.relu4(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x) # Output layer (raw scores/logits)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš¡ LightningModule: Encapsulating Model & Training Logic\n",
    "\n",
    "A `pytorch_lightning.LightningModule` organizes the PyTorch code related to training,\n",
    "validation, and testing. It encapsulates:\n",
    "- The model architecture (our CNN or ResNet).\n",
    "- The forward pass logic.\n",
    "- The calculation of the loss.\n",
    "- The code performed during training, validation, and test steps.\n",
    "- The configuration of optimizers and learning rate schedulers.\n",
    "\n",
    "This makes the code cleaner, more reusable, and integrates seamlessly with the\n",
    "Lightning `Trainer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistologyClassifier(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    PyTorch Lightning Module for histology image classification.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 model_name: str = 'resnet18', # Or 'custom_cnn'\n",
    "                 num_classes: int = NUM_CLASSES,\n",
    "                 learning_rate: float = LEARNING_RATE,\n",
    "                 optimizer: str = 'AdamW',\n",
    "                 lr_scheduler: str = 'ReduceLROnPlateau',\n",
    "                 pretrained: bool = True): # Added pretrained flag\n",
    "        super().__init__()\n",
    "        # Save hyperparameters for logging and potential loading later\n",
    "        # Important: Don't save the actual model instance here if it's large or complex\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Initialize the chosen model architecture\n",
    "        if model_name == 'resnet18':\n",
    "            self.model = PretrainedResNetGAP(num_classes=num_classes, backbone='resnet18', pretrained=pretrained)\n",
    "        elif model_name == 'resnet34':\n",
    "             self.model = PretrainedResNetGAP(num_classes=num_classes, backbone='resnet34', pretrained=pretrained)\n",
    "        elif model_name == 'custom_cnn':\n",
    "            self.model = SimpleHistologyCNN(num_classes=num_classes)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model_name: {model_name}\")\n",
    "\n",
    "        # Loss function (CrossEntropyLoss is common for multi-class classification)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Example placeholder for metrics (we'll add proper ones in Task 4.3)\n",
    "        # from torchmetrics.classification import MulticlassAccuracy\n",
    "        # self.accuracy = MulticlassAccuracy(num_classes=num_classes)\n",
    "        metric_args = {'num_classes': num_classes, 'average': 'macro'}\n",
    "\n",
    "        self.train_metrics = torchmetrics.MetricCollection({\n",
    "            'acc': MulticlassAccuracy(**metric_args),\n",
    "            'precision': MulticlassPrecision(**metric_args),\n",
    "            'recall': MulticlassRecall(**metric_args),\n",
    "            'f1': MulticlassF1Score(**metric_args)\n",
    "            # AUC requires probabilities or logits\n",
    "            # 'auroc': MulticlassAUROC(**metric_args, thresholds=None) # Use default thresholds\n",
    "        })\n",
    "        self.val_metrics = self.train_metrics.clone(prefix='val_') # Clone with prefix\n",
    "        self.test_metrics = self.train_metrics.clone(prefix='test_') # Clone with prefix\n",
    "\n",
    "        # Separate AUROC metric as it requires logits/probabilities directly\n",
    "        auroc_args = {'num_classes': num_classes, 'average': 'macro', 'thresholds': None}\n",
    "        self.train_auroc = MulticlassAUROC(**auroc_args)\n",
    "        self.val_auroc = MulticlassAUROC(**auroc_args)\n",
    "        self.test_auroc = MulticlassAUROC(**auroc_args)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The forward pass is simply delegated to the underlying model\n",
    "        return self.model(x)\n",
    "\n",
    "    def _calculate_loss(self, batch, batch_idx, stage='train'):\n",
    "        images, labels = batch\n",
    "        logits = self.forward(images)\n",
    "        loss = self.criterion(logits, labels)\n",
    "\n",
    "        # --- Log Loss ---\n",
    "        self.log(f'{stage}_loss_step', loss, on_step=True, on_epoch=False, prog_bar=False, logger=True)\n",
    "        self.log(f'{stage}_loss_epoch', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        # --- Calculate & Log Metrics ---\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        # Apply softmax to get probabilities for AUROC\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "\n",
    "        # Select the correct metric collection based on the stage\n",
    "        if stage == 'train':\n",
    "            metrics_obj = self.train_metrics\n",
    "            auroc_obj = self.train_auroc\n",
    "        elif stage == 'val':\n",
    "            metrics_obj = self.val_metrics\n",
    "            auroc_obj = self.val_auroc\n",
    "        elif stage == 'test':\n",
    "            metrics_obj = self.test_metrics\n",
    "            auroc_obj = self.test_auroc\n",
    "        else:\n",
    "             raise ValueError(f\"Invalid stage: {stage}\")\n",
    "\n",
    "        # Update metrics\n",
    "        metrics_obj.update(preds, labels)\n",
    "        auroc_obj.update(probs, labels) # AUROC uses probabilities/logits\n",
    "\n",
    "        # Log metrics at the end of the epoch\n",
    "        self.log_dict(metrics_obj, on_step=False, on_epoch=True, logger=True)\n",
    "        self.log(f'{stage}_auroc', auroc_obj, on_step=False, on_epoch=True, logger=True) # Log AUROC separately\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Defines the logic for a single training step\n",
    "        return self._calculate_loss(batch, batch_idx, stage='train')\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Defines the logic for a single validation step\n",
    "        # This is used to monitor performance on unseen data during training\n",
    "        return self._calculate_loss(batch, batch_idx, stage='val')\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Defines the logic for a single testing step\n",
    "        # This is used after training to evaluate the final model performance\n",
    "        return self._calculate_loss(batch, batch_idx, stage='test')\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Choose optimizers and learning-rate schedulers to use during training.\n",
    "        \"\"\"\n",
    "        if self.hparams.optimizer.lower() == 'adamw':\n",
    "            optimizer = AdamW(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        elif self.hparams.optimizer.lower() == 'adam':\n",
    "             optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        # Add other optimizers like SGD if needed\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported optimizer: {self.hparams.optimizer}\")\n",
    "\n",
    "        # Configure learning rate scheduler if specified\n",
    "        if self.hparams.lr_scheduler.lower() == 'reducelronplateau':\n",
    "            # Reduce learning rate when validation loss plateaus\n",
    "            scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "            return {\n",
    "                'optimizer': optimizer,\n",
    "                'lr_scheduler': {\n",
    "                    'scheduler': scheduler,\n",
    "                    'monitor': 'val_loss_epoch', # Metric to monitor\n",
    "                    'interval': 'epoch',\n",
    "                    'frequency': 1\n",
    "                }\n",
    "            }\n",
    "        elif self.hparams.lr_scheduler.lower() == 'none':\n",
    "            return optimizer # Return only the optimizer if no scheduler\n",
    "        else:\n",
    "             raise ValueError(f\"Unsupported lr_scheduler: {self.hparams.lr_scheduler}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics for Evaluation\n",
    "\n",
    "Choosing appropriate metrics is crucial for evaluating classification performance,\n",
    "especially in medical imaging where class imbalance can be common. We use:\n",
    "\n",
    "- **Accuracy:** Overall percentage of correct predictions. Can be misleading if classes are imbalanced.\n",
    "- **Precision:** Of the samples predicted as positive, how many actually were? (TP / (TP + FP)). High precision means fewer false positives. Important when the cost of a false positive is high.\n",
    "- **Recall (Sensitivity):** Of the actual positive samples, how many were correctly identified? (TP / (TP + FN)). High recall means fewer false negatives. Important when the cost of missing a positive case is high.\n",
    "- **F1-Score:** The harmonic mean of precision and recall (2 * (Precision * Recall) / (Precision + Recall)). Provides a balance between precision and recall.\n",
    "- **AUC (Area Under the ROC Curve):** Measures the model's ability to distinguish between classes across all possible thresholds. An AUC of 1.0 is perfect, 0.5 is random guessing. Often a good overall measure, especially for imbalanced datasets.\n",
    "\n",
    "We use `'macro'` averaging, which calculates the metric independently for each class and then takes the unweighted average. This treats all classes equally, regardless of their size. `'weighted'` averaging considers class support.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
